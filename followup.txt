1. How would you evaluate your autosuggest server? If you made another version, how would you compare the two to decide which is better?
	
A decent way is to take a look at when and if a suggestion was taken. A mean character count, that is, how many characters in average did a person have to type until a sentence was selected, may be a good way to evaluate this.

2. One way to improve the autosuggest server is to give topic-specific suggestions. How would you design an auto-categorization server? It should take a list of messages and return a TopicId. (Assume that every conversation in the training set has a TopicId).
	
I would get a list of conversations with their topics. I can run a series of word counts and n-grams to create a series of columns on each topic. Using this, I can create a classification model to predict which topic any given conversation is. Once I have a conversation, I can figure out a topic id (or at least a probability that it is any given topic) and adjust the recommendation list accordingly.	

3. How would you evaluate if your auto-categorization server is good?

The simplest would be to figure out a mean-squared error measure for the characterization of topics. That is, figure out how many times you figure out the topic correctly. In theory, if you correctly predict the topic, you should be able to have better recommendations.

4. Processing hundreds of millions of conversations for your autosuggest and auto-categorize models could take a very long time. How could you distribute the processing across multiple machines?

Depending on the models, you could easily parallelize the processes for both of these algorithms. For example, if you ran a random forest, or any other ensemble -based algorithm, running the procedure for each individual algorithm and combining them later, would be a typical case of map-reduce. You'd want to make sure that the algorithms are set up in such a way that an optimal load-balancing structure was achieved among the servers. 


